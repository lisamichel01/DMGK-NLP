{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ac3bbf",
   "metadata": {},
   "source": [
    "# Processing texts using spaCy\n",
    "\n",
    "*Content adapated from [Tuomo Hiipala'](https://www.mv.helsinki.fi/home/thiippal/) lectures\n",
    "\n",
    "This section introduces you to basic tasks in natural language processing and how they can be performed using a Python library named spaCy.\n",
    "\n",
    "After this section, you should:\n",
    "\n",
    " - know some of the key concepts and tasks in natural language processing\n",
    " - know how to perform simple natural language processing tasks using the spaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a232d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the spaCy library (if not installed, let's do it)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eeee63",
   "metadata": {},
   "source": [
    "To perform natural language processing tasks for a given language, we must load a _language model_ that has been trained to perform these tasks for the language in question. \n",
    "\n",
    "spaCy supports [many languages](https://spacy.io/usage/models#languages), but provides pre-trained [language models](https://spacy.io/models/) for fewer languages.\n",
    "\n",
    "These language models come in different sizes and flavours. We will explore these models and their differences later. \n",
    "\n",
    "To get acquainted with basic tasks in natural language processing, we will start with a small language model for the English language.\n",
    "\n",
    "Language models are loaded using spaCy's `load()` function, which takes the name of the model as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small language model for English and assign it to the variable 'nlp'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Call the variable to examine the object\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cb00c",
   "metadata": {},
   "source": [
    "### What is a language model?\n",
    "\n",
    "Most modern language models are based on *statistics* instead of human-defined rules. \n",
    "\n",
    "Statistical language models are based on probabilities, e.g.: \n",
    "\n",
    " - What is the probability of a given sentence occurring in a language? \n",
    " - How likely is a given word to occur in a sequence of words?\n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "> From financial exchanges in `HIDDEN` Manhattan to cloakrooms in Washington and homeless shelters in California, unfamiliar rituals were the order of the day.\n",
    "\n",
    "> Security precautions were being taken around the `HIDDEN` as the deadline for Iraq to withdraw from Kuwait neared.\n",
    "\n",
    "You can probably make informed guesses on the `HIDDEN` words based on your knowledge of the English language and the world in general.\n",
    "\n",
    "Similarly, creating a statistical language model involves observing the occurrence of words in large corpora and calculating their probabilities of occurrence in a given context. The language model can then be trained by making predictions and adjusting the model based on the errors made during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a87a15",
   "metadata": {},
   "source": [
    "### How are language models trained?\n",
    "\n",
    "The small language model for English, for instance, is trained on a corpus called [OntoNotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19), which features texts from different *genres* such as newswire text, broadcast news, broadcast and telephone conversations and blogs.\n",
    "\n",
    "This allows the corpus to cover linguistic variation in both written and spoken English.\n",
    "\n",
    "The OntoNotes 5.0 corpus consists of more than just *plain text*: the annotations include *part-of-speech tags*, *syntactic dependencies* and *co-references* between words.\n",
    "\n",
    "This allows modelling not just the occurrence of particular words or their sequences, but their grammatical features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98233c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the text again\n",
    "with open('data/treaty_of_lisbon.txt', 'r', encoding='UTF-8') as f:\n",
    "    text = f.read()[335:577] # just to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a81075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we feed the 'text' to the language object under 'nlp' and\n",
    "# store the result under the variable 'doc'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3db872",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c50f10",
   "metadata": {},
   "source": [
    "What takes place first is a task known as *tokenization*, which breaks the text down into analytical units in need of further processing. \n",
    "\n",
    "In most cases, a *token* corresponds to words separated by whitespace, but punctuation marks are also considered as independent tokens. Because computers treat words as sequences of characters, assigning punctuation marks to their own tokens prevents trailing punctuation from attaching to the words that precede them.\n",
    "\n",
    "The diagram below the outlines the tasks that spaCy can perform after a text has been tokenised, such as *part-of-speech tagging*, *syntactic parsing* and *named entity recognition*.\n",
    "\n",
    "![The spaCy pipeline from https://spacy.io/usage/linguistic-features#section-tokenization](data/spacy_pipeline.png)\n",
    "\n",
    "A spaCy _Doc_ object is consists of a sequence of *Token* objects, which store the results of various natural language processing tasks.\n",
    "\n",
    "Let's print out each *Token* object stored in the _Doc_ object `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over items in the Doc object, using the variable 'token' to refer to items in the list\n",
    "for token in doc:  \n",
    "    \n",
    "    # Print each token\n",
    "    print(token)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9230ef",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f430e0",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms.\n",
    "\n",
    "Consider the example: *The sailor dogs the hatch*.\n",
    "\n",
    "The present tense of the verb *dog* (to fasten something with something) is precisely the same as the plural form of the noun *dog*: *dogs*.\n",
    "\n",
    "To identify the correct word class, we must examine the context in which the word appears.\n",
    "\n",
    "*spaCy* provides two types of part-of-speech tags, *coarse* and *fine-grained*, which are stored under the attributes `pos_` and `tag_`, respectively.\n",
    "\n",
    "We can access the attributes of a Python object by inserting the *attribute* after the *object* and separating them with a full stop, e.g. `token.pos_`.\n",
    "\n",
    "To access the results of POS tagging, let's loop over the *Doc* object `doc` and print each *Token* and its coarse and fine-grained part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5dd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping again over items in the Doc object, using the variable 'token' to refer to items in the list\n",
    "for token in doc:\n",
    "    \n",
    "    # Print the token and the POS tags\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af90589",
   "metadata": {},
   "source": [
    "The coarse part-of-speech tags available under the `pos_` attribute are based on the [Universal Dependencies](https://universaldependencies.org/u/pos/all.html) tag set.\n",
    "\n",
    "The fine-grained part-of-speech tags under `tag_`, in turn, are based on the OntoNotes 5.0 corpus introduced above.\n",
    "\n",
    "In contrast to coarse part-of-speech tags, the fine-grained tags also encode [grammatical information](https://spacy.io/api/annotation#pos-en). The tags for verbs, for example, are distinguished by aspect and tense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b082f",
   "metadata": {},
   "source": [
    "### Morphological analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d9b92",
   "metadata": {},
   "source": [
    "Morphemes constitute the smallest grammatical units that carry meaning. Two types of morphemes are generally recognised: *free* morphemes, which consist of words that can stand on their own, and *bound* morphemes, which inflect other morphemes. For the English language, bound morphemes include suffixes such as _-s_, which is used to indicate the plural form of a noun.\n",
    "\n",
    "Put differently, morphemes shape the external *form* of a word, and these forms are associated with given grammatical *functions*.\n",
    "\n",
    "spaCy performs morphological analysis automatically and stores the result under the attribute `morph` of a _Token_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping once more over items in the Doc object, using the variable 'token' to refer to items in the list\n",
    "for token in doc:\n",
    "\n",
    "    # Print the token and the results of morphological analysis\n",
    "    print(f'{token} --> {token.morph}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12c53d",
   "metadata": {},
   "source": [
    "As the output shows, not all _Tokens_ have morphological information, because they consist of free morphemes.\n",
    "\n",
    "To retrieve morphological information from a _Token_ object, we must use the `get()` method of the `morph` attribute.\n",
    "\n",
    "We can use the brackets `[]` to access items in the _Doc_ object.\n",
    "\n",
    "The following line retrieves morphological information about aspect for the 22nd _Token_ in the _Doc_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving morphological information \n",
    "doc[16].morph.get('Aspect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token without morphological information\n",
    "doc[10].morph.get('Aspect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf326f3",
   "metadata": {},
   "source": [
    "### Syntactic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over (again) items in the Doc object, using the variable 'token' to refer to items in the list\n",
    "for token in doc:\n",
    "    \n",
    "    # Print the token and its dependency tag\n",
    "    print(token, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce627c",
   "metadata": {},
   "source": [
    "Unlike part-of-speech tags that are associated with a single _Token_, dependency tags indicate a relation that holds between two *Tokens*.\n",
    "\n",
    "To better understand the syntactic relations captured by dependency parsing, let's use some of the additional attributes available for each *Token*:\n",
    "\n",
    " 1. `i`: the position of the *Token* in the *Doc*\n",
    " 2. `token`: the *Token* itself\n",
    " 3. `dep_`: a tag for the syntactic relation\n",
    " 4. `head` and `i`: the *Token* that governs the current *Token* and its index\n",
    " \n",
    "This illustrates how Python attributes can be used in a flexible manner: the attribute `head` points to another *Token*, which naturally has the attribute `i` that contains its index or position in the *Doc*. We can combine these two attributes to retrieve this information for any token by referring to `.head.i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    \n",
    "    # Print the index of current token, the token itself, the dependency, the head and its index\n",
    "    print(token.i, token, token.dep_, token.head.i, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de1468",
   "metadata": {},
   "source": [
    "Although the output above helps to clarify the syntactic dependencies between tokens, they are generally much easier to perceive using diagrams.\n",
    "\n",
    "spaCy provides a [visualisation tool](https://spacy.io/usage/visualizers) for visualising dependencies. This component of the spaCy library, _displacy_, can be imported using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7450f8",
   "metadata": {},
   "source": [
    "The `displacy` module has a method named `render()`, which takes a _Doc_ object as input.\n",
    "\n",
    "To draw a dependency tree, we provide the _Doc_ object `doc` to the `render()` method with two arguments:\n",
    "\n",
    " 1. `style`: The value `dep` instructs _displacy_ to draw a visualisation for syntactic dependencies.\n",
    " 2. `options`: This argument takes a Python dictionary as input. We provide a dictionary with the key `compact` and Boolean value `True` to instruct _displacy_ to draw a compact tree diagram. Additional options for formatting the visualisation can be found in spaCy [documentation](https://spacy.io/api/top-level#displacy_options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', options={'compact': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = nlp('Last night we planned on meeting for Mexican food to celebrate a friends birthday.')\n",
    "displacy.render(doc_test, style='dep', options={'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf2d0c",
   "metadata": {},
   "source": [
    "The syntactic dependencies are visualised using lines that lead from the **head** *Token* to the *Token* governed by that head.\n",
    "\n",
    "The dependency tags are based on [Universal Dependencies](https://universaldependencies.org/), a framework for describing morphological and syntactic features across languages (for a theoretical discussion of Universal Dependencies, see de Marneffe et al. [2021](https://doi.org/10.1162/coli_a_00402)).\n",
    "\n",
    "If you don't know what a particular tag means, spaCy provides a function for explaining the tags, `explain()`, which takes a tag as input (note that the tags are case-sensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('npadvmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f97b6",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ee92a",
   "metadata": {},
   "source": [
    "A lemma is the base form of a word. Keep in mind that unless explicitly instructed, computers cannot tell the difference between singular and plural forms of words, but treat them as distinct tokens, because their forms differ.\n",
    "\n",
    "If one wants to count the occurrences of words, for instance, a process known as lemmatization is needed to group together the different forms of the same token.\n",
    "\n",
    "Lemmas are available for each Token under the attribute lemma_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ce2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    \n",
    "    # Print the token and its dependency tag\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915dd02c",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb425c4",
   "metadata": {},
   "source": [
    "1. Load the `treaty_of_lisbon.txt` file  \n",
    "2. Extract `Article 8 A` from there  \n",
    "3. Feed text to the spaCy language model\n",
    "4. Capture the verbs and the nouns\n",
    "5. Find the frequency of the verbs and nouns (using lemmatization) in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab69b6",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5745d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('data/treaty_of_lisbon.txt', 'r', encoding='UTF-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "ind_begin = text.find('Article 8 A')\n",
    "ind_end = text.find('Article 8 B')\n",
    "\n",
    "text = text[ind_begin:ind_end]\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29858b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_and_verbs_dict = {'nouns':[], 'verbs':[]}\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns_and_verbs_dict['nouns'].append(token.lemma_)\n",
    "    if token.pos_ == 'VERB':\n",
    "        nouns_and_verbs_dict['verbs'].append(token.lemma_)\n",
    "        \n",
    "print(nouns_and_verbs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(nouns_and_verbs_dict['nouns']))\n",
    "print(Counter(nouns_and_verbs_dict['verbs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab05929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
