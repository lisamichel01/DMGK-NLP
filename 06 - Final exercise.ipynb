{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data from r/AmItheAsshole\n",
    "2. Create a dictionary with the id as key and the text as value of each submission (the first 500)\n",
    "3. Get the entities of each text in a dictionary with the submission id as key and the entities in a list as value\n",
    "4. Create a dictionary id as key and sentiment analysis (using NaiveBaiesAnalyzer) as value (first 50)\n",
    "5. Apply gensim's LDA topic model\n",
    "6. Check the TF-IDF value of a few words in a few texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob \n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/amitheasshole_corpus.json', 'r', encoding='UTF-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the dictionary with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for sub in data[:500]:\n",
    "    corpus[sub['id']] = sub['selftext'].replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {}\n",
    "for ID,text in corpus.items():\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ent_list = []\n",
    "    for ent in doc.ents:\n",
    "        ent_list.append((ent.text,ent.label_))\n",
    "        \n",
    "    entities[ID] = ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing a topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the texts into tokens:\n",
    "tokenized_corpus = {}\n",
    "\n",
    "for ID,text in corpus.items():\n",
    "        \n",
    "    # create the spacy doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # filtering tokens and lemmatizing\n",
    "    proc_text = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num:\n",
    "            proc_text.append(token.lemma_.lower())\n",
    "\n",
    "    tokenized_corpus[ID] = proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the corpus into bag of words\n",
    "# mapping words to ids\n",
    "words_id = corpora.Dictionary(tokenized_corpus.values())\n",
    "\n",
    "# corpus becomes a bag of words\n",
    "corpus_lda = [words_id.doc2bow(txt) for txt in tokenized_corpus.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking coherence score\n",
    "\n",
    "# checking \"optimal\" number of topics\n",
    "k_init = 5\n",
    "k_final = 15\n",
    "for k in range(k_init,k_final+1):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus_lda,\n",
    "                                           id2word=words_id,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=50,\n",
    "                                           passes=20,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    # let's compute perplexity (lower) and coherence score (higher)\n",
    "    per_lda = lda_model.log_perplexity(corpus_lda)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_corpus.values(), dictionary=words_id, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(k,per_lda,coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running lda model for number of topics with highest cohrence score\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus_lda,\n",
    "                                           id2word=words_id,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=50,\n",
    "                                           passes=20,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composition\n",
    "lda_model.show_topics(num_words=20,num_topics=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding to NLP pipeline\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {}\n",
    "for ID in list(corpus.keys())[:50]:\n",
    "    \n",
    "    blob = TextBlob(corpus[ID], analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "    sent_dict[ID] = blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the vocabulary of the corpus\n",
    "word_list = [token for text in tokenized_corpus.values() for token in text]\n",
    "word_set = set(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size and number of texts\n",
    "vocab_size = len(word_set)\n",
    "n_texts = len(tokenized_corpus)  # it should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size, n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting frequency of words\n",
    "word_count = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Term Frequency (TF)\n",
    "def termFreq(word,text):\n",
    "    \n",
    "    N = len(text)\n",
    "    F = Counter(text)[word]\n",
    "    \n",
    "    return F/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Inverse Document Frequency (IDF)\n",
    "IDF_dict = {}\n",
    "\n",
    "for word in word_set:\n",
    "    count = 0\n",
    "    for text in tokenized_corpus.values():\n",
    "        if word in text:\n",
    "            count += 1\n",
    "            \n",
    "    IDF_dict[word] = np.log(n_texts/count)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the TF-IDF of a word in a text\n",
    "for ID,text in tokenized_corpus.items():\n",
    "    if 'car' in text:\n",
    "        print(corpus[ID])\n",
    "        print(termFreq('car',text)/IDF_dict['car'])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
